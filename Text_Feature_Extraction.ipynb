{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOqojjaEEIBJuLDPIzYotkt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sahel-Eskandar/Text-Mining-Classification-Analysis/blob/main/Text_Feature_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MdLiWi9vvDA",
        "outputId": "582f4803-6591-4d9a-b865-9db551d2dd2a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6UhwAJpqwXCP"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import NMF, PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "import spacy\n",
        "\n",
        "# Input text\n",
        "text = \"Natural Language Processing (NLP) is a subfield of computer science, \" \\\n",
        "       \"artificial intelligence, and computational linguistics concerned with \" \\\n",
        "       \"the interactions between computers and human (natural) languages. \" \\\n",
        "       \"It focuses on how to program computers to process and analyze large \" \\\n",
        "       \"amounts of natural language data.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokens), tokens[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7kW7Uio0rwe",
        "outputId": "0733cea7-90a7-4496-ad18-650fb6e978aa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50 ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'computer', 'science', ',', 'artificial', 'intelligence', ',', 'and', 'computational', 'linguistics', 'concerned']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CountVectorizer\n",
        "count_vec = CountVectorizer()\n",
        "X_count = count_vec.fit_transform([text])\n",
        "print('CountVectorizer:')\n",
        "print(count_vec.get_feature_names_out()[:10])\n",
        "print(X_count.toarray()[0][:10])"
      ],
      "metadata": {
        "id": "NJ68L0vQwer7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8d75ec0-da62-4742-ad1d-e7a63bd4cf4a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVectorizer:\n",
            "['amounts' 'analyze' 'and' 'artificial' 'between' 'computational'\n",
            " 'computer' 'computers' 'concerned' 'data']\n",
            "[1 1 3 1 1 1 1 2 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "tfidf_vec = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vec.fit_transform([text])\n",
        "print('TF-IDF:')\n",
        "print(tfidf_vec.get_feature_names_out()[:10])\n",
        "print(X_tfidf.toarray()[0][:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEcRpvb3yE3r",
        "outputId": "60c275fe-6b65-4cb6-b4d4-7282700253c7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF:\n",
            "['amounts' 'analyze' 'and' 'artificial' 'between' 'computational'\n",
            " 'computer' 'computers' 'concerned' 'data']\n",
            "[0.12803688 0.12803688 0.38411064 0.12803688 0.12803688 0.12803688\n",
            " 0.12803688 0.25607376 0.12803688 0.12803688]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word embeddings (using spaCy)\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(text)\n",
        "embeddings = [token.vector for token in doc]\n",
        "print('Word embeddings:')\n",
        "print(len(embeddings))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LP9bT3C5zw5t",
        "outputId": "189e1a68-874d-4de3-8656-194927cf54d4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word embeddings:\n",
            "50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare similarities using cosine similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Similarity between \"Natural\" and \"Language\"\n",
        "word1 = \"natural\"\n",
        "word2 = \"language\"\n",
        "embedding_sim = cosine_similarity(embeddings[tokens.index(word1)].reshape(1, -1),\n",
        "                                   embeddings[tokens.index(word2)].reshape(1, -1))\n",
        "tfidf_sim = cosine_similarity(X_tfidf[:, tfidf_vec.vocabulary_[word1]].reshape(1, -1),\n",
        "                               X_tfidf[:, tfidf_vec.vocabulary_[word2]].reshape(1, -1))\n",
        "print(f'Similarity between \"{word1}\" and \"{word2}\" using word embeddings:', embedding_sim[0][0])\n",
        "print(f'Similarity between \"{word1}\" and \"{word2}\" using TF-IDF:', tfidf_sim[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0kaod41vcNy",
        "outputId": "30439b04-2587-465c-bb37-bb6f4a5007a6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between \"natural\" and \"language\" using word embeddings: 0.23813576\n",
            "Similarity between \"natural\" and \"language\" using TF-IDF: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag of words\n",
        "bag_of_words = {word: tokens.count(word) for word in set(tokens)}\n",
        "print('Bag of words:')\n",
        "print(list(bag_of_words.items())[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZ1-Yz0Azz2_",
        "outputId": "a3eb5902-a1ea-4f93-9724-12c28906e1b2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of words:\n",
            "[('a', 1), ('data', 1), ('concerned', 1), ('natural', 2), ('analyze', 1), ('intelligence', 1), ('to', 2), ('interactions', 1), ('Language', 1), ('computers', 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag of n-grams\n",
        "n = 2\n",
        "ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
        "bag_of_ngrams = {ngram: ngrams.count(ngram) for ngram in set(ngrams)}\n",
        "print('Bag of n-grams:')\n",
        "print(list(bag_of_ngrams.items())[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XJ7avHGz1kG",
        "outputId": "8e375b5a-ba68-43bf-f67b-8f70850cdd5c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of n-grams:\n",
            "[(('(', 'natural'), 1), (('linguistics', 'concerned'), 1), (('program', 'computers'), 1), (('amounts', 'of'), 1), (('Natural', 'Language'), 1), (('and', 'analyze'), 1), (('with', 'the'), 1), (('.', 'It'), 1), (('between', 'computers'), 1), (('NLP', ')'), 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HashingVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "hash_vec = HashingVectorizer(n_features=100)\n",
        "X_hash = hash_vec.fit_transform([text])\n",
        "print('HashingVectorizer:')\n",
        "print(X_hash.shape, X_hash.toarray()[0][:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8311qJ7tyG3g",
        "outputId": "6a9d2de8-a201-4b8c-86d0-1e46ae4e6849"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HashingVectorizer:\n",
            "(1, 100) [ 0.          0.         -0.13483997  0.          0.          0.13483997\n",
            "  0.          0.          0.         -0.13483997]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Latent Dirichlet Allocation (LDA)\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
        "X_lda = lda.fit_transform(X_tfidf)\n",
        "print('LDA:')\n",
        "print(X_lda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IM_DO1Bn4CLD",
        "outputId": "218fd292-37b2-4005-a07b-405579c4f261"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LDA:\n",
            "[[0.01600128 0.01600128 0.01600128 0.01600128 0.01600128 0.85598847\n",
            "  0.01600128 0.01600128 0.01600128 0.01600128]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Non-negative Matrix Factorization (NMF)\n",
        "nmf = NMF(n_components=10, random_state=42)\n",
        "X_nmf = nmf.fit_transform(X_tfidf)\n",
        "print('NMF:')\n",
        "print(X_nmf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9CfuopG4IS8",
        "outputId": "065144b1-8b7d-4ab1-8ee9-0877e0f3b5b9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NMF:\n",
            "[[6.55551346e-01 4.28578416e-01 1.47212861e-02 2.40446075e-16\n",
            "  1.54113864e-01 1.05308938e-01 5.73261840e-02 1.65705288e-01\n",
            "  1.43820169e-01 2.48592727e-16]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text into sentences and words\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "words = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "# Perform POS tagging on the words\n",
        "pos_tags = [nltk.pos_tag(sentence) for sentence in words]\n",
        "\n",
        "# Print the POS tags\n",
        "for sentence in pos_tags:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-M_OU0XDWNY",
        "outputId": "ba65c61a-649a-4005-9471-78a51cd2bf15"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('subfield', 'NN'), ('of', 'IN'), ('computer', 'NN'), ('science', 'NN'), (',', ','), ('artificial', 'JJ'), ('intelligence', 'NN'), (',', ','), ('and', 'CC'), ('computational', 'JJ'), ('linguistics', 'NNS'), ('concerned', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('interactions', 'NNS'), ('between', 'IN'), ('computers', 'NNS'), ('and', 'CC'), ('human', 'JJ'), ('(', '('), ('natural', 'JJ'), (')', ')'), ('languages', 'NNS'), ('.', '.')]\n",
            "[('It', 'PRP'), ('focuses', 'VBZ'), ('on', 'IN'), ('how', 'WRB'), ('to', 'TO'), ('program', 'NN'), ('computers', 'NNS'), ('to', 'TO'), ('process', 'VB'), ('and', 'CC'), ('analyze', 'VB'), ('large', 'JJ'), ('amounts', 'NNS'), ('of', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('data', 'NNS'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-xOQrMg5VRnv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}